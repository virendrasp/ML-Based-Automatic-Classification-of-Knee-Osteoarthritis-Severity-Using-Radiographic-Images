{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05_Model_Evaluation.ipynb\n",
    "\n",
    "# Introduction\n",
    "# This notebook will evaluate the trained models using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC\n",
    "\n",
    "# Importing Libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading Trained Models\n",
    "from tensorflow.keras.models import load_model\n",
    "cnn_model = load_model('models/cnn_model.h5')\n",
    "\n",
    "# Evaluation Metrics\n",
    "y_pred = cnn_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Visualizing Results\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curves\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
